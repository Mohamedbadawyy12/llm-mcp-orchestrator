# app/api/routes_chat.py

import json
from fastapi import APIRouter, HTTPException
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, BaseMessage
from pydantic import BaseModel
from sse_starlette.sse import EventSourceResponse
from loguru import logger
from typing import Dict, List

from app.core import orchestrator

router = APIRouter()

class ChatRequest(BaseModel):
    """Request model for chat streaming."""
    message: str
    thread_id: str  # This is the key to our conversation memory


# This is our simple in-memory "database" for conversation histories.
conversation_memory: Dict[str, List[BaseMessage]] = {}


@router.post("/chat/stream")
async def chat_stream(chat_request: ChatRequest):
    """Streams agent responses using Server-Sent Events (SSE)."""
    if not orchestrator.agent_graph:
        raise HTTPException(status_code=500, detail="Orchestrator not initialized. Check startup logs.")

    logger.info(f"Received chat request: '{chat_request.message}' [thread={chat_request.thread_id}]")

    # 1. Get the old conversation history from memory
    current_history = conversation_memory.get(chat_request.thread_id, [])

    # 2. Add the new user message to the history
    current_messages = current_history + [HumanMessage(content=chat_request.message)]
    
    # 3. Prepare the input for the graph
    graph_input = {"messages": current_messages}

    # This list will store new messages (from the AI) to be saved back to memory
    new_messages_from_graph: List[BaseMessage] = []

    async def event_stream():
        try:
            # Run the graph stream with the full message history
            async for step in orchestrator.agent_graph.astream_events(graph_input, version="v1"):
                
                if step["event"] != "on_chain_end":
                    continue 

                data = step["data"].get("output")
                if not (isinstance(data, dict) and "messages" in data):
                    continue

                messages = data["messages"]
                for msg in messages:
                    
                    if isinstance(msg, AIMessage):
                        
                        new_messages_from_graph.append(msg) 
                        
                        if msg.tool_calls:
                            logger.debug("Skipping (AIMessage with tool calls)...")
                            continue 
                        
                        final_content = ""
                        if isinstance(msg.content, str):
                            final_content = msg.content
                        elif isinstance(msg.content, list):
                            for part in msg.content:
                                if isinstance(part, dict) and "text" in part:
                                    final_content += part["text"]
                        
                        # ---------------------------------------------------------
                        # ğŸ’¡ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ÙˆØ­ÙŠØ¯ Ù‡Ù†Ø§
                        # ---------------------------------------------------------
                        # Ø¨Ø¯Ù„ Ù…Ø§ Ù†Ø¨Ø¹Øª Ø§Ù„Ù€ JSON object ÙƒÙ„Ù‡
                        # yield json.dumps(clean_data_to_send)
                        
                        # Ù‡Ù†Ø¨Ø¹Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (final_content) Ø¨Ø³
                        # `EventSourceResponse` Ù‡ÙŠØªÙˆÙ„Ù‰ ØªØºÙ„ÙŠÙÙ‡ ÙƒÙ€ data: "..."
                        if final_content:
                            yield json.dumps(final_content)
                        # ---------------------------------------------------------

                    elif isinstance(msg, ToolMessage):
                        new_messages_from_graph.append(msg)
                        logger.debug("Skipping (ToolMessage)...")
                        continue 

        except Exception as e:
            logger.error(f"Error during agent execution: {e}")
            yield json.dumps({"error": str(e)})
        
        finally:
            # Ø­ÙØ¸ Ø§Ù„Ù€ History
            if new_messages_from_graph:
                conversation_memory[chat_request.thread_id] = current_messages + new_messages_from_graph
                logger.success(f"Updated memory for thread {chat_request.thread_id}. Total messages: {len(conversation_memory[chat_request.thread_id])}")
            else:
                logger.warning(f"No new messages generated by graph. Memory for thread {chat_request.thread_id} not updated.")

    return EventSourceResponse(event_stream())